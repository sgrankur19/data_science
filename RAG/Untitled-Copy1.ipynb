{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a260c65-4825-43c5-b47f-a5694f2e1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "\n",
    "#pip install langchain openai faiss-cpu chromadb tiktoken unstructured pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdec310a-6afb-43b0-be7d-716fdeba205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-community\n",
    "#pip install transformers\n",
    "#pip install sentence_transformers\n",
    "#pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5655f16-d9f6-4102-a00b-5a116a6204f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bafeb0fe-17d2-4f5f-874a-a8c85d948de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS, Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da50d687-2b4d-4451-8090-9ca2c6f213f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ae83828-9996-4403-8c10-600a694a9f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 722 documents\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Documents\n",
    "\n",
    "documents = \"\"\"Problem: ValueError: The feature names should match those that were passed during fit. \n",
    "Summarizing: The error ValueError: The feature names should match those that were passed during fit. usually means that the columns in your x DataFrame do not exactly match the feature names that your model or transformer was trained on. \n",
    "Possible causes: The columns in x after one-hot encoding do not exactly match feature_names (missing or extra columns). The order of columns may be different. Some expected columns are missing from your current data. \n",
    "How to debug/fix: Add a check to compare x.columns and feature_names before reindexing. You can also fill missing columns with zeros to ensure all expected features are present.\"\"\"\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c689d9-15d8-4c76-bd04-c712bafa8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Chunk Documents\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,     # size of each chunk\n",
    "    chunk_overlap=20,   # overlap between chunks for context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ead1efd5-fed3-4f7d-8ae1-f5b52fb3447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 4\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Problem: ValueError: The feature names should match those that were passed during fit.\n",
      "\n",
      "--- Chunk 2 ---\n",
      "\n",
      "Summarizing: The error ValueError: The feature names should match those that were passed during fit. usually means that the columns in your x DataFrame do not exactly match the feature names that your model or transformer was trained on. \n",
      "\n",
      "--- Chunk 3 ---\n",
      "\n",
      "Possible causes: The columns in x after one-hot encoding do not exactly match feature_names (missing or extra columns). The order of columns may be different. Some expected columns are missing from your current data. \n",
      "\n",
      "--- Chunk 4 ---\n",
      "\n",
      "How to debug/fix: Add a check to compare x.columns and feature_names before reindexing. You can also fill missing columns with zeros to ensure all expected features are present.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(documents)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"--- Chunk {i} ---\")\n",
    "    print(chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e96c29d-f771-4456-9285-0a77c888e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(page_content=chunk, metadata={\"source\": \"custom_text\"}) \n",
    "             for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0378fc5-3e12-4dad-8acd-abddbf9e0c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qp/g8gjs5xx2937j_gh9pqg5cgc0000gn/T/ipykernel_4767/223694980.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# 4. Create Embeddings + Vector Store\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1de33f83-f259-41af-a774-3bb68596ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56f3e3f4-fae7-4553-b54c-05dae194f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create Retriever\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fe7373-8b1b-46d9-b1b7-6244cae7b92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18f3be72d1d4648b5fdaa0f8755caff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455eccf9843f46c8b67cc8a0703aa00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee2936c87754c07b27b23559da68a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1769fe9ee5c4cbbbd221b882a4934bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87b97b9c4094b83a9cd4a3ea7cd62c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6. Build RAG Chain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"tiiuae/falcon-7b-instruct\",\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c54255-2243-4765-9da2-c2ec1172d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Query the RAG System\n",
    "\n",
    "query = \"How to fix the error ValueError: The feature names should match those that were passed during fit.\"\n",
    "result = qa_chain(query)\n",
    "\n",
    "print(\"\\n Question:\", query)\n",
    "print(\"\\n Answer:\", result[\"result\"])\n",
    "print(\"\\n Sources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7242dd1-d7ff-45a9-a49c-c437da6ff3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3771e70a-98ab-42cc-b4f2-be65c5120b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd5440-130b-4e5b-a778-41db82f37fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e367304-45b4-4a95-84bc-a301fa811e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288a463-bf55-4d17-ab08-4486c9ae95f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863b352-6b7f-477e-ab11-1ff7a76f6973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
